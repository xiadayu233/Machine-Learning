{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Gradient Descent?\n",
    "it is an optimization algorithm used to find the minimum of a function. We randomly pick a starting point, then take steps in the negative direction of the gradient to reach the local/global minima\n",
    "\n",
    "1. take the gradient of the function\n",
    "2. pick random values for the starting point\n",
    "3. plug the parameter values in the gradient\n",
    "4. calculate the step size, $step size = \\alpha \\times  gradient$\n",
    "5. calculate the new parameters: $new params = old params - step size$\n",
    "6. go back to step 3 and repete until the step size is small enough or you reach the maximum number of steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we need to use gradient descent?\n",
    "1. sometimes we don't have the explit solution for $gradient = 0$, we need to find the zero point step by step\n",
    "2. it's very painful to solve the function $gradient = 0$ analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn:  SGD\n",
    "https://scikit-learn.org/stable/modules/sgd.html\n",
    "Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.\n",
    "\n",
    "#### 1. The concrete loss function cans be set via loss parameter:\n",
    "* loss = \"hinge\": (soft-margin) linear Support Vector Machine;\n",
    "* loss = \"log\": logistic regression\";\n",
    "* loss = \"squared_loss\": Ordinary least squares\n",
    "\n",
    "#### 2. The concrete penalty can be set via the penalty parameter:\n",
    "* penalty=\"l2\": L2 norm penalty on coef_.\n",
    "* penalty=\"l1\": L1 norm penalty on coef_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note it is a classifier instead of regression model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.91080278, 9.91080278]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
